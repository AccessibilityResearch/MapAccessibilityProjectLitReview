#Christian bibs
@inproceedings{10.1145/2207676.2207734,
author = {Yatani, Koji and Banovic, Nikola and Truong, Khai},
title = {SpaceSense: Representing Geographical Information to Visually Impaired People Using Spatial Tactile Feedback},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207734},
doi = {10.1145/2207676.2207734},
abstract = {Learning an environment can be challenging for people with visual impairments. Braille maps allow their users to understand the spatial relationship between a set of places. However, physical Braille maps are often costly, may not always cover an area of interest with sufficient detail, and might not present up-to-date information. We built a handheld system for representing geographical information called SpaceSense, which includes custom spatial tactile feedback hardware-multiple vibration motors attached to different locations on a mobile touch-screen device. It offers high-level information about the distance and direction towards a destination and bookmarked places through vibrotactile feedback to help the user maintain the spatial relationships between these points. SpaceSense also adapts a summarization technique for online user reviews of public and commercial venues. Our user study shows that participants could build and maintain the spatial relationships between places on a map more accurately with SpaceSense compared to a system without spatial tactile feedback. They pointed specifically to having spatial tactile feedback as the contributing factor in successfully building and maintaining their mental map.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {415–424},
numpages = {10},
keywords = {handheld devices, geographical information representation, users with visual impairments, touch screens, vibrotactile feedback, assistive technology},
location = {Austin, Texas, USA},
series = {CHI '12}
}
@inproceedings{10.1145/1851600.1851606,
author = {Su, Jing and Rosenzweig, Alyssa and Goel, Ashvin and de Lara, Eyal and Truong, Khai N.},
title = {Timbremap: Enabling the Visually-Impaired to Use Maps on Touch-Enabled Devices},
year = {2010},
isbn = {9781605588353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851600.1851606},
doi = {10.1145/1851600.1851606},
abstract = {Mapping applications on mobile devices have gained widespread popularity as a means for enhancing user mobility and ability to explore new locations and venues. Visually impaired users currently rely on computer text-to-speech or human-spoken descriptions of maps and indoor spaces. Unfortunately, speech-based descriptions are limited in their ability to succinctly convey complex layouts or spacial positioning.This paper presents Timbremap, a sonification interface enabling visually impaired users to explore complex indoor layouts using off-the-shelf touch-screen mobile devices. This is achieved using audio feedback to guide the user's finger on the device's touch interface to convey geometry. Our user-study evaluation shows Timbremap is effective in conveying non-trivial geometry and enabling visually impaired users to explore indoor layouts.},
booktitle = {Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {17–26},
numpages = {10},
keywords = {sonification, assistive, touch device, user interface},
location = {Lisbon, Portugal},
series = {MobileHCI '10}
}
@article{10.1145/3186894, author = {G\"{o}tzelmann, T.}, title = {Visually Augmented Audio-Tactile Graphics for Visually Impaired People}, year = {2018}, issue_date = {June 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {11}, number = {2}, issn = {1936-7228}, url = {https://doi.org/10.1145/3186894}, doi = {10.1145/3186894}, abstract = {Tactile graphics play an essential role in knowledge transfer for blind people. The tactile exploration of these graphics is often challenging because of the cognitive load caused by physiological constraints and their complexity. The coupling of physical tactile graphics with electronic devices offers to support the tactile exploration by auditory feedback. Often, these systems have strict constraints regarding their mobility or the process of coupling both components. Additionally, visually impaired people cannot appropriately benefit from their residual vision. This article presents a concept for 3D printed tactile graphics, which offers to use audio-tactile graphics with usual smartphones or tablet-computers. By using capacitive markers, the coupling of the tactile graphics with the mobile device is simplified. These tactile graphics integrating these markers can be printed in one turn by off-the-shelf 3D printers without any post-processing and allows us to use multiple elevation levels for graphical elements. Based on the developed generic concept on visually augmented audio-tactile graphics, we presented a case study for maps. A prototypical implementation was tested by a user study with visually impaired people. All the participants were able to interact with the 3D printed tactile maps using a standard tablet computer. To study the effect of visual augmentation of graphical elements, we conducted another comprehensive user study. We tested multiple types of graphics and obtained evidence that visual augmentation may offer clear advantages for the exploration of tactile graphics. Even participants with a minor residual vision could solve the tasks with visual augmentation more quickly and accurately.}, journal = {ACM Trans. Access. Comput.}, month = {jun}, articleno = {8}, numpages = {31}, keywords = {visually impaired, blind, global, marker, touch screen, capacitive, orientation, Tactile graphics, audio-tactile, accessibility, capacitive sensing, augmented, 3D printing, tangible user interfaces}
}
@article{COBO2017294,
title = {Differences between blind people's cognitive maps after proximity and distant exploration of virtual environments},
journal = {Computers in Human Behavior},
volume = {77},
pages = {294-308},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217305332},
author = {Antonio Cobo and Nancy E. Guerrón and Carlos Martín and Francisco {del Pozo} and José Javier Serrano},
keywords = {Cognitive mapping, Blind people, Virtual reality, Smartphone},
abstract = {Visits to simulations of real spaces in virtual reality have been proposed as a means for blind people to gain spatial knowledge regarding the disposition of obstacles within a place before actually visiting it. Within the present study, different configurations of distant and proximity exploration were compared to each other, in order to test whether differences in effectiveness and efficiency lead to changes in exploration behaviour, without a detrimental impact on cognitive-map quality and usefulness. Evidence supports effectiveness improvements due to distant exploration (p-value = 0.0006). The flat-spotlight distant-configuration entails a 53% reduction in discovery time (p-value = 0.0027). A trend is observed entailing a 38% reduction in the duration of the overall exploration stage for a flat spotlight configuration (p-value = 0.067). Wall-detection effectiveness alters exploration duration (p-value = 0.012). Improvements in effectiveness and discovery time are associated with shorter overall exploration time. Duration of exploration after discovery time depends on wall-detection effectiveness. Benefits from a distant exploration configuration are not enough to build better cognitive maps.}
}
@Article{electronics10080953,
AUTHOR = {Oh, Uran and Joh, Hwayeon and Lee, YunJung},
TITLE = {Image Accessibility for Screen Reader Users: A Systematic Review and a Road Map},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {953},
URL = {https://www.mdpi.com/2079-9292/10/8/953},
ISSN = {2079-9292},
ABSTRACT = {A number of studies have been conducted to improve the accessibility of images using touchscreen devices for screen reader users. In this study, we conducted a systematic review of 33 papers to get a holistic understanding of existing approaches and to suggest a research road map given identified gaps. As a result, we identified types of images, visual information, input device and feedback modalities that were studied for improving image accessibility using touchscreen devices. Findings also revealed that there is little study how the generation of image-related information can be automated. Moreover, we confirmed that the involvement of screen reader users is mostly limited to evaluations, while input from target users during the design process is particularly important for the development of assistive technologies. Then we introduce two of our recent studies on the accessibility of artwork and comics, AccessArt and AccessComics, respectively. Based on the identified key challenges, we suggest a research agenda for improving image accessibility for screen reader users.},
DOI = {10.3390/electronics10080953}
}
@inproceedings{10.1145/3173574.3173772,
author = {Holloway, Leona and Marriott, Kim and Butler, Matthew},
title = {Accessible Maps for the Blind: Comparing 3D Printed Models with Tactile Graphics},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173772},
doi = {10.1145/3173574.3173772},
abstract = {Tactile maps are widely used in Orientation and Mobility (O&amp;M) training for people with blindness and severe vision impairment. Commodity 3D printers now offer an alternative way to present accessible graphics, however it is unclear if 3D models offer advantages over tactile equivalents for 2D graphics such as maps. In a controlled study with 16 touch readers, we found that 3D models were preferred, enabled the use of more easily understood icons, facilitated better short term recall and allowed relative height of map elements to be more easily understood. Analysis of hand movements revealed the use of novel strategies for systematic scanning of the 3D model and gaining an overview of the map. Finally, we explored how 3D printed maps can be augmented with interactive audio labels, replacing less practical braille labels. Our findings suggest that 3D printed maps do indeed offer advantages for O&amp;M training.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {vision impairment, orientation and mobility training, mapping, blindness, 3d printing, accessibility},
location = {Montreal QC, Canada},
series = {CHI '18}
}
#Aaron Cherney .bib file
@article{doi:10.1080/10447318.2017.1279827,
author = {Radu-Daniel Vatavu},
title = {Visual Impairments and Mobile Touchscreen Interaction: State-of-the-Art, Causes of Visual Impairment, and Design Guidelines},
journal = {International Journal of Human–Computer Interaction},
volume = {33},
number = {6},
pages = {486-509},
year  = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/10447318.2017.1279827},

URL = { 
        https://doi.org/10.1080/10447318.2017.1279827
    
},
eprint = { 
        https://doi.org/10.1080/10447318.2017.1279827
    
}

}

@inproceedings{10.1145/2468356.2468364,
author = {Abd Hamid, Nazatul Naquiah and Edwards, Alistair D.N.},
title = {Facilitating Route Learning Using Interactive Audio-Tactile Maps for Blind and Visually Impaired People},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468364},
doi = {10.1145/2468356.2468364},
abstract = {In preparing to navigate in an unfamiliar location, a blind person may use non-visual maps. This project is aimed at developing more effective, interactive audio-tactile maps. The maps will be novel in using speech and non-speech sounds and allowing the user to rotate the map, thereby facilitating the building of an egocentric cognitive map. Initial requirements have been gathered from mobility instructors. Their main conclusions are that immoveable objects represent the most useful landmarks and that certain ambient sounds can provide most valuable orientation information.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {37–42},
numpages = {6},
keywords = {orientation, touch, audio-tactile maps, route learning, blindness, visual impairment, speech, auditory icons, multimodal, accessibility, tactile maps},
location = {Paris, France},
series = {CHI EA '13}
}

@article{10.1145/2815169.2815171,
author = {Medina, Jonathas Leontino and Cagnin, Maria Istela and Paiva, D\'{e}bora Maria Barroso},
title = {Investigating Accessibility on Web-Based Maps},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2815169.2815171},
doi = {10.1145/2815169.2815171},
abstract = {This paper presents results of an accessibility evaluation carried out with web-based map applications. Three points of view were considered: experts on accessibility, evaluation tools and final users (partially or totally blind people). The document WCAG 2.0 (Web Content Accessibility Guidelines) provided us with guidelines for evaluation and GQM (Goal, Question and Metric) approach was used to define and set measurable goals. A number of problems was identified and none of the evaluated applications entirely meet the analyzed criteria.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {aug},
pages = {17–26},
numpages = {10},
keywords = {web-based maps, accessibility evaluation, web accessibility, WCAG 2.0}
}

@InProceedings{10.1007/978-3-319-41267-2_20,
author="G{\"o}tzelmann, Timo",
editor="Miesenberger, Klaus
and B{\"u}hler, Christian
and Penaz, Petr",
title="CapMaps",
booktitle="Computers Helping People with Special Needs",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="146--152",
abstract="Tactile maps can be useful tools for blind people for navigation and orientation tasks. Apart from static maps, there are techniques to augment tactile maps with audio content. They can be used to interact with the map content, to offer extra information and to reduce the tactile complexity of a map. Studies show that audio-tactile maps can be more efficient and satisfying for the user than pure tactile maps without audio feedback. A major challenge of audio-tactile maps is the linkage of tactile elements with audio content and interactivity. This paper introduces a novel approach to link 3D printed tactile maps with mobile devices, such as smartphones and tablets, in a flexible way to enable interactivity and audio-support. By integrating conductive filaments into the printed maps it seamlessly integrates into the 3D printing process. This allows to automatically recognize the tactile map by a single press at its corner. Additionally, the arrangement of the tactile map on the mobile device is flexible and detected automatically which eases the use of these maps. The practicability of this approach is shown by a dedicated feasibility study.",
isbn="978-3-319-41267-2"
}

@article{RODRIGUEZSANCHEZ20147210,
title = {Accessible smartphones for blind users: A case study for a wayfinding system},
journal = {Expert Systems with Applications},
volume = {41},
number = {16},
pages = {7210-7222},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S095741741400311X},
author = {M.C. Rodriguez-Sanchez and M.A. Moreno-Alvarez and E. Martin and S. Borromeo and J.A. Hernandez-Tamames},
keywords = {Accessibility, Visually impaired users, User interfaces, Touch screens, Wayfinding},
abstract = {While progress on assistive technologies have been made, some blind users still face several problems opening and using basic functionalities when interacting with touch interfaces. Sometimes, people with visual impairments may also have problems navigating autonomously, without personal assistance, especially in unknown environments. This paper presents a complete solution to manage the basic functions of a smartphone and to guide users using a wayfinding application. This way, a blind user could go to work from his home in an autonomous way using an adaptable wayfinding application on his smartphone. The wayfinding application combines text, map, auditory and tactile feedback for providing the information. Eighteen visually impaired users tested the application. Preliminary results from this study show that blind people and limited vision users can effectively use the wayfinding application without help. The evaluation also confirms the usefulness of extending the vibration feedback to convey distance information as well as directional information. The validation was successful for iOS and Android devices.}
}

@InProceedings{10.1007/978-3-319-07440-5_15,
author="Neto, Jos{\'e} Monserrat
and Freire, Andr{\'e} P.
and Souto, Sabrina S.
and Ab{\'i}lio, Ramon S.",
editor="Stephanidis, Constantine
and Antona, Margherita",
title="Usability Evaluation of a Web System for Spatially Oriented Audio Descriptions of Images Addressed to Visually Impaired People",
booktitle="Universal Access in Human-Computer Interaction. Universal Access to Information and Knowledge",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="154--165",
abstract="This paper describes a web system designed to provide spatially oriented audio descriptions of an image for visually impaired users. The system uses a hardware-independent platform of the technique of multimodal presentation of images. Visually impaired users interact with an image displayed on the screen while moving the cursor -- with a mouse or a tablet (pen or finger touch) -- and listening to the audio description of previously marked areas within the image. The paper also describes the usability evaluation performed with five participants and its main results. Generally, the five participants accomplished the usability test tasks and could better understand the image displayed. The paper also describes the main findings and discusses some implications for design, suggesting some improvements.",
isbn="978-3-319-07440-5"
}

@article{LAHIB201816,
title = {Evaluating Fitts’ law on vibrating touch-screen to improve visual data accessibility for blind users},
journal = {International Journal of Human-Computer Studies},
volume = {112},
pages = {16-27},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918300053},
author = {Manahel El Lahib and Joe Tekli and Youssef Bou Issa},
keywords = {Blind users, Pointing method, Visual data accessibility, Fitts’ law, Vibrating touch-screen},
abstract = {The pointing task is the process of pointing to an object on a computer monitor using a pointing device, or physically touching an object with the hand or finger. It is an important element for users when manipulating visual computer interfaces such as traditional screens and touch-screens. In this context, Fitts’ Law remains one of the central studies that have mathematically modeled the pointing method, and was found to be a good predictor of the average time needed to perform a pointing task. Yet, in our modern computerized society, accessing visual information becomes a central need for all kinds of users, namely users who are blind or visually impaired. Hence, the goal of our study is to evaluate whether Fitts’ Law can be applied for blind candidates using the vibration modality on a touch-screen. To achieve this, we first review the literature on Fitts’ Law and visual data accessibility solutions for blind users. Then, we introduce an experimental framework titled FittsEVAL studying the ability of blind users to tap specific shapes on a touch-screen, while varying different parameters, namely: target distance, target size, and the angle of attack of the pointing task. Experiments on blindfolded and blind candidates show that Fitts’ Law can be effectively applied for blind users using a vibrating touch-screen under certain parameters (i.e., when varying target distance and size), while it is not verified under others (i.e., when varying the angle of attack). This can be considered as a first step toward a more complete experimental evaluation of vibrating touch-screen accessibility, toward designing more adapted interfaces for the blind.}
}

@inproceedings{10.1145/3373625.3416999,
author = {Sharif, Ather and Pao, Victoria and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {The Reliability of Fitts’s Law as a Movement Model for People with and without Limited Fine Motor Function},
year = {2020},
isbn = {9781450371032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3416999},
doi = {10.1145/3373625.3416999},
abstract = {For over six decades, Fitts’s law (1954) has been utilized by researchers to quantify human pointing performance in terms of “throughput,” a combined speed-accuracy measure of aimed movement efficiency. Throughput measurements are commonly used to evaluate pointing techniques and devices, helping to inform software and hardware developments. Although Fitts’s law has been used extensively in HCI and beyond, its test-retest reliability, both in terms of throughput and model fit, from one session to the next, is still unexplored. Additionally, despite the fact that prior work has shown that Fitts’s law provides good model fits, with Pearson correlation coefficients commonly at r=.90 or above, the model fitness of Fitts’s law has not been thoroughly investigated for people who exhibit limited fine motor function in their dominant hand. To fill these gaps, we conducted a study with 21 participants with limited fine motor function and 34 participants without such limitations. Each participant performed a classic reciprocal pointing task comprising vertical ribbons in a 1-D layout in two sessions, which were at least four hours and at most 48 hours apart. Our findings indicate that the throughput values between the two sessions were statistically significantly different, both for people with and without limited fine motor function, suggesting that Fitts’s law provides low test-retest reliability. Importantly, the test-retest reliability of Fitts’s throughput metric was 4.7% lower for people with limited fine motor function. Additionally, we found that the model fitness of Fitts’s law as measured by Pearson correlation coefficient, r, was .89 (SD=0.08) for people without limited fine motor function, and .81 (SD=0.09) for people with limited fine motor function. Taken together, these results indicate that Fitts’s law should be used with caution and, if possible, over multiple sessions, especially when used in assistive technology evaluations.},
booktitle = {The 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {16},
numpages = {15},
keywords = {throughput, models, Fitts’s law, model fitness, test-retest reliability, mouse},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{10.1145/274644.274681,
author = {Friedlander, Naomi and Schlueter, Kevin and Mantei, Marilyn},
title = {Bullseye! When Fitts' Law Doesn't Fit},
year = {1998},
isbn = {0201309874},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/274644.274681},
doi = {10.1145/274644.274681},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {257–264},
numpages = {8},
location = {Los Angeles, California, USA},
series = {CHI '98}
}

@inproceedings{10.1145/1168987.1169008,
author = {Cohen, Robert F. and Haven, Valerie and Lanzoni, Jessica A. and Meacham, Arthur and Skaff, Joelle and Wissell, Michael},
title = {Using an Audio Interface to Assist Users Who Are Visually Impaired with Steering Tasks},
year = {2006},
isbn = {1595932909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168987.1169008},
doi = {10.1145/1168987.1169008},
abstract = {In this paper we describe the latest results in our on-going study of techniques to present relational graphs to users with visual impairments. Our work tests the effectiveness of the PLUMB software package, which uses audio feedback and the pen-based Tablet PC interface to relay graphs and diagrams to users with visual impairments. Our study included human trials with ten participants without usable vision, in which we evaluated the users' ability to perform steering tasks under varying conditions.},
booktitle = {Proceedings of the 8th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {119–124},
numpages = {6},
keywords = {accessibility, graph, sonification, audio},
location = {Portland, Oregon, USA},
series = {Assets '06}
}
#Karter bibs
@article{watanabe2017evaluation,
  title={Evaluation of virtual tactile dots on touchscreens in map reading: Perception of distance and direction},
  author={Watanabe, Tetsuya and Kaga, Hirotsugu and Yagi, Tsubasa},
  journal={Journal of Advanced Computational Intelligence and Intelligent Informatics},
  volume={21},
  number={1},
  pages={79--86},
  year={2017},
  publisher={Fuji Technology Press Ltd.}
}
@article{dobrivsek2003evolution,
  title={Evolution of the information-retrieval system for blind and visually-impaired people},
  author={Dobri{\v{s}}ek, Simon and Gros, Jerneja and Vesnicer, Bo{\v{s}}tjan and others},
  journal={International Journal of Speech Technology},
  volume={6},
  number={3},
  pages={301--309},
  year={2003},
  publisher={Springer}
}
@article{thompson2018examination,
  title={Examination of the Level of Inclusion of Blind Subjects in the Development of Touchscreen Accessibility Technologies},
  author={Thompson, Robert Christopher},
  year={2018}
}
@inproceedings{grussenmeyer2016feasibility,
  title={Feasibility of using haptic directions through maps with a tablet and smart watch for people who are blind and visually impaired},
  author={Grussenmeyer, William and Garcia, Jesel and Jiang, Fang},
  booktitle={Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
  pages={83--89},
  year={2016}
}
@misc{bandyopadhyay2017sound,
  title={The sound and feel of titrations: A smartphone aid for color-blind and visually impaired students},
  author={Bandyopadhyay, Subhajit and Rathod, Balraj B},
  year={2017},
  publisher={ACS Publications}
}